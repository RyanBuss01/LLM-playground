{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer, LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT - J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"models/GPT-J\"\n",
    "\n",
    "# Load the model\n",
    "# model = GPTJForCausalLM.from_pretrained(model_name)\n",
    "device = \"cuda\"\n",
    "model_GPT = GPTJForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    revision=\"float16\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model_GPT.cuda()\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer_GPT = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of life? The simple question has been asked and answered many times. If there was one answer, it would obviously have some sort of cosmic importance. No, the meaning of life can’t be something so simple as to be based solely on a person’s religious doctrine. There must be more to it than that. The meaning of life is complex and multifaceted. It can never be fully explained by only one factor, no matter how central or important.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the meaning of life?\"\n",
    "\n",
    "input_ids_GPT = tokenizer_GPT(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "gen_tokens = model_GPT.generate(\n",
    "    input_ids_GPT.to(device),\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=100,\n",
    ")\n",
    "gen_text = tokenizer_GPT.batch_decode(gen_tokens)[0]\n",
    "\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear GPU memory\n",
    "del model_GPT\n",
    "del input_ids_GPT\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.84s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"models/Llama-2-7B-fp16\"\n",
    "\n",
    "# Load the model\n",
    "# model = GPTJForCausalLM.from_pretrained(model_name)\n",
    "device = \"cuda\"\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    revision=\"float16\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.cuda()\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_llama(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.to('cuda')\n",
    "    output = model.generate(input_ids, max_length=256, num_beams=4, no_repeat_ngram_size=2)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"What is the meaning of life?\"\n",
    "\n",
    "response = chat_with_llama(input)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear GPU memory\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Pajama - Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n"
     ]
    }
   ],
   "source": [
    "model_name = \"models/redp-7b-instruct\"\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\"\n",
    "# init\n",
    "tokenizer_red = AutoTokenizer.from_pretrained(model_name)\n",
    "model_red = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "model_red.to(device)\n",
    "# infer\n",
    "question = \"The capital of France is?\"\n",
    "prompt = f\"Q: {question}?\\nA:\"\n",
    "inputs = tokenizer_red(prompt, return_tensors='pt').to(model_red.device)\n",
    "input_length = inputs.input_ids.shape[1]\n",
    "outputs = model_red.generate(\n",
    "    **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n",
    ")\n",
    "token = outputs.sequences[0, input_length:]\n",
    "output_str = tokenizer_red.decode(token)\n",
    "\n",
    "def truncate_at_first_delimiter(s, delimiters):\n",
    "    # Find the earliest occurrence of any delimiter\n",
    "    first_pos = min((s.find(d) for d in delimiters if s.find(d) != -1), default=-1)\n",
    "    # Return the truncated string or the original if no delimiter was found\n",
    "    return s[:first_pos] if first_pos != -1 else s\n",
    "\n",
    "delimiters = ['Q:', 'q:', 'Question:', 'question:']\n",
    "truncated_string = truncate_at_first_delimiter(output_str, delimiters)\n",
    "print(truncated_string.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
